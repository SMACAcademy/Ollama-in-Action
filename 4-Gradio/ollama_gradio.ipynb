{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56b22d-880c-4dc1-8325-3aea0f31511b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "!pip install langchain chromadb gradio \n",
    "!pip install -U langchain-community\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09504989-e4da-443c-98c0-db67fc15f22f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import ollama  \n",
    "import gradio as gr  \n",
    "\n",
    "# Document processing and retrieval  \n",
    "from langchain_community.document_loaders import PyMuPDFLoader  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain.vectorstores import Chroma  \n",
    "\n",
    "# Embedding generation  \n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea826cef-9dba-4441-bf66-f50098955fe1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain embeddings and vectors in AI. Hmm, where do I start? Well, I know that in machine learning, data is often represented as features or nodes in a neural network. But what exactly are embeddings?\n",
      "\n",
      "I think embeddings are like a way to convert text into numerical representations. Wait, but isn't that similar to word embeddings? Yeah, maybe they're used in areas like natural language processing. So instead of just having a one-hot vector for each word, which can be inefficient, embeddings create continuous vectors where each word's vector captures its meaning.\n",
      "\n",
      "So, for example, if I have the words \"man\", \"woman\", and \"apple\", their embeddings would reflect that \"woman\" is related to \"man\" because they share similar components. And \"apple\" might have different components altogether, maybe not as related to the others.\n",
      "\n",
      "Vectors, on the other hand, are like these continuous representations. They can be of any dimension but usually lower than the input space. They capture relationships between data points. In machine learning models, vectors are used to make predictions or classifications. So embeddings are just a specific type of vector that captures semantic information, right?\n",
      "\n",
      "Wait, how do they work in practice? I remember something about using matrices to map words to vectors. That's probably the skip-gram model or something similar from Word2Vec. Each word is assigned a unique vector based on its context.\n",
      "\n",
      "But what are their limitations? Maybe high-dimensional spaces can be problematic because models need efficient computation. Also, if the data is not well structured, embeddings might not capture the right relationships. Plus, there's the issue of interpretability—people might not understand how these vectors represent concepts.\n",
      "\n",
      "Applications... Natural Language Processing definitely uses them a lot. Maybe in text generation models or recommendation systems where understanding the semantics helps make better suggestions. Computer vision could use embeddings for image features, and in other areas like anomaly detection, they help identify patterns.\n",
      "\n",
      "Comparing them to other representations, one-hot encoding is just binary and not very useful for continuous operations. So embeddings are better because they offer a dense representation that's more efficient and useful for tasks needing semantic understanding.\n",
      "\n",
      "So, in summary, embeddings are numerical representations of data that capture their semantics, while vectors are these continuous representations used across various ML tasks. They help in reducing dimensionality and improving model performance by capturing meaningful relationships.\n",
      "</think>\n",
      "\n",
      "**Explanation of Embeddings and Vectors in AI**\n",
      "\n",
      "**1. Introduction to Embeddings:**\n",
      "Embeddings are a type of representation technique used in machine learning, particularly in areas like Natural Language Processing (NLP). They convert text into numerical vectors that capture the semantic meaning of data. Unlike one-hot encoding, which is inefficient due to its sparsity, embeddings create continuous vectors. Each word is assigned a unique vector, reflecting its context and relationships with other words.\n",
      "\n",
      "**2. Understanding Vectors:**\n",
      "Vectors are continuous representations used across various AI tasks. They can be of any dimension but are typically lower than the input space's dimensionality. These vectors capture essential information about data points, enabling efficient computation in models for tasks like classification or prediction.\n",
      "\n",
      "**3. Mechanism and Examples:**\n",
      "In practice, embeddings often use matrices to map words (or data points) into vector form. For instance, the word \"woman\" might have an embedding close to \"man,\" indicating a semantic relationship. This is evident in techniques like Word2Vec's skip-gram model.\n",
      "\n",
      "**4. Limitations and Considerations:**\n",
      "While embeddings are powerful, they face challenges such as high-dimensional spaces requiring efficient computation and interpretability issues. Misuse can lead to incorrect relationships being captured if the data isn't well-structured.\n",
      "\n",
      "**5. Applications:**\n",
      "Embeddings are widely used in NLP for tasks like text generation and recommendation systems. They also find applications in computer vision for image features and in anomaly detection by identifying patterns based on semantic understanding.\n",
      "\n",
      "**6. Comparison with Other Representations:**\n",
      "Compared to one-hot encoding, embeddings offer a dense, efficient representation that's more suitable for continuous operations, enhancing model performance by capturing meaningful relationships.\n",
      "\n",
      "In summary, embeddings transform data into meaningful numerical vectors, while vectors are versatile tools used across AI tasks. Together, they contribute significantly to models by reducing dimensionality and improving semantic understanding.\n"
     ]
    }
   ],
   "source": [
    "# Call the Ollama model to generate a response  \n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:8b\",  \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Embeddings and Vectors in AI.\"},  # User's input query\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response[\"message\"][\"content\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d2b0f7-a3ec-418f-9807-20b6ddba252d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the function that processes the PDF\n",
    "def process_pdf(pdf_bytes):\n",
    "    if pdf_bytes is None:\n",
    "        return None, None, None\n",
    "\n",
    "    loader = PyMuPDFLoader(pdf_bytes) \n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "\n",
    "    vectorstore=Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\")  # Example directory\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \"\"\"\n",
    "    The function returns 3 objects\n",
    "        text_splitter → (Used to split new text in the same way as before)\n",
    "        vectorstore → (Holds the processed document chunks)\n",
    "        retriever → (Used to fetch relevant document chunks when answering questions)\n",
    "    \"\"\"\n",
    "    \n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdce7c97-8c46-402f-a574-42139ad67509",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5d31ca-42ba-4c0e-8f7b-5233e776dc98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ollama_llm(question, context):\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"deepseek-r1:8b\",  # Specifies the AI model to use\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]  # Formats the user input\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "\n",
    "    final_answer = re.sub(r'<think>.*?</think>', # We're searching for think tags\n",
    "                          '', # We'll replace them with empty spaces\n",
    "                          response_content, # In response_content\n",
    "                          flags=re.DOTALL).strip() # (dot) should match newlines (\\n) as well.\n",
    "    # Return the final cleaned response\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf97fc9-c2a1-45c0-b716-56c5a9d30708",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define rag_chain function for Retrieval Augmented Generation\n",
    "def rag_chain(question, text_splitter, vectorstore, retriever):\n",
    "    \"\"\"\n",
    "    This function takes as input:\n",
    "        - The question we want to ask the model\n",
    "        - The text_splitter object to split the PDF and read into chunks\n",
    "        - The vectorstore for retrieving embeddings \n",
    "        - The retriever objects which retrieves data from the vectorstore\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(question) \n",
    "    formatted_content = combine_docs(retrieved_docs) \n",
    "    return ollama_llm(question, formatted_content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6a285-8920-4717-a33d-972c3e54e19d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ask_question(pdf_bytes, question): \n",
    "    text_splitter, vectorstore, retriever = process_pdf(pdf_bytes) # Process the PDF\n",
    "    if text_splitter is None:\n",
    "        return None  # No PDF uploaded    \n",
    "    result = rag_chain(question, text_splitter, vectorstore, retriever) # Return the results with RAG\n",
    "    #return {result}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbdfcc2-466b-4190-8711-b9ebce57b1b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\site-packages\\gradio\\blocks.py\", line 1912, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\site-packages\\gradio\\components\\markdown.py\", line 124, in postprocess\n",
      "    unindented_y = inspect.cleandoc(value)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Muthu\\.conda\\envs\\langchain_py311\\Lib\\inspect.py\", line 869, in cleandoc\n",
      "    lines = doc.expandtabs().split('\\n')\n",
      "            ^^^^^^^^^^^^^^\n",
      "AttributeError: 'set' object has no attribute 'expandtabs'\n"
     ]
    }
   ],
   "source": [
    "# Define a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,  \n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF (optional)\"),  \n",
    "        gr.Textbox(label=\"Ask a question\")  \n",
    "    ],\n",
    "    #outputs=\"text\",  \n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Ask questions about your PDF\",  \n",
    "    description=\"Use Ollama model to answer your questions about the uploaded PDF document.\",  \n",
    ")\n",
    "\n",
    "# Launch the Gradio interface to start the web-based app\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
