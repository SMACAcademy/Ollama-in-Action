{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b56b22d-880c-4dc1-8325-3aea0f31511b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "!pip install langchain chromadb gradio \n",
    "!pip install -U langchain-community\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09504989-e4da-443c-98c0-db67fc15f22f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import ollama  \n",
    "import gradio as gr  \n",
    "\n",
    "# Document processing and retrieval  \n",
    "from langchain_community.document_loaders import PyMuPDFLoader  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain.vectorstores import Chroma  \n",
    "\n",
    "# Embedding generation  \n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "\n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea826cef-9dba-4441-bf66-f50098955fe1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to explain embeddings and vectors in AI. Hmm, where do I start? Well, I remember from my basic AI courses that data is often represented in vector form. But what exactly are embeddings? They sound a bit like vectors, but maybe they're something more specific.\n",
      "\n",
      "I think embeddings have to do with converting text into numbers. Oh right, word embeddings! Like how each word is turned into a vector where the dimensions correspond to some concept. So, maybe it's about capturing semantic meanings or something? I'm not entirely sure. Let me try to break this down.\n",
      "\n",
      "Vectors in AI are like arrays of numbers, right? They represent data points in a multi-dimensional space. So, if I have a picture, its pixels form a vector. In text, words might be represented as vectors too. But how do embeddings fit into this?\n",
      "\n",
      "Wait, maybe embeddings are a type of vector representation specifically for text. So, each word is mapped to a high-dimensional vector. These vectors capture the word's position in the language, or something like that. I think it's called Word2Vec or something similar.\n",
      "\n",
      "But isn't there more than just words? Maybe phrases or sentences also have embeddings. Oh right, embeddings can be for any kind of data, not just text. Like images or even sequences of data over time. So, they're a way to compress complex data into vectors that capture their essential characteristics.\n",
      "\n",
      "Why are embeddings important? Well, because vectors are hard to work with directly if they're high-dimensional. So, embeddings might help in making these vectors more manageable for algorithms. Maybe it's about how models understand different types of data by translating them into similar vector spaces.\n",
      "\n",
      "I'm a bit fuzzy on the difference between embeddings and other representations. I think another term is feature extraction—extracting meaningful features from data and representing them as vectors. So, embeddings are a way to represent complex data in a compact form that's useful for AI tasks like classification or recommendation systems.\n",
      "\n",
      "How do models use these embeddings? Probably by feeding them into neural networks where they can be processed together. It makes sense because vectors can be easily manipulated with matrix operations, which is crucial for machine learning algorithms.\n",
      "\n",
      "But I'm still not entirely clear on how the dimensions of these vectors are determined. Is it based on the data's complexity or some arbitrary choice? Maybe there's a balance between the number of features and the model's capacity to learn effectively.\n",
      "\n",
      "Wait, I think there's something called latent representations as well. Are embeddings a type of latent representation? They seem similar because they capture essential information about the data without explicitly storing all details.\n",
      "\n",
      "So, putting it all together: vectors are multi-dimensional arrays that help in representing complex data structures like images or text. Embeddings take this a step further by converting these structures into compact vector representations that capture their key features. This makes them easier for models to process and analyze.\n",
      "\n",
      "I should probably check some examples. For text, each word becomes a vector; for images, each pixel group might become an embedding. These embeddings are then used in various AI tasks where the model needs to understand and relate different pieces of data effectively.\n",
      "\n",
      "Wait, but how does this differ from something like Bag of Words (BoW)? I think BoW is another way of representing text as vectors based on word frequencies, but it's more of a statistical method. Embeddings, on the other hand, might be more sophisticated, capturing semantic relationships through training processes like Word2Vec.\n",
      "\n",
      "So, in summary: Vectors are used across various AI tasks to represent data points in a multi-dimensional space. Embeddings take this a step further by converting complex data structures into dense vector representations that capture their characteristics. This makes it easier for models to understand and use these features effectively in different applications.\n",
      "</think>\n",
      "\n",
      "In the realm of Artificial Intelligence, vectors and embeddings are fundamental concepts that facilitate the processing and analysis of various types of data. Here's an organized explanation:\n",
      "\n",
      "1. **Vectors**: \n",
      "   - Vectors are multi-dimensional arrays of numbers, representing data points in a higher-dimensional space. They are crucial for tasks like image recognition, where pixel values form a vector, or text processing, where words might be converted into vectors.\n",
      "\n",
      "2. **Embeddings**:\n",
      "   - Embeddings extend the concept of vectors by converting complex data structures (like text, images) into dense vector representations that capture their essential characteristics.\n",
      "   - They are particularly useful for text, where each word is mapped to a high-dimensional vector. These embeddings often capture semantic meanings and are trained using techniques like Word2Vec.\n",
      "\n",
      "3. **Applications**:\n",
      "   - Embeddings simplify the processing of complex data by translating them into compact vectors, making it easier for AI models to understand and utilize these features in tasks such as classification or recommendation systems.\n",
      "\n",
      "4. **Comparison with Other Representations**:\n",
      "   - While similar to Bag of Words (BoW), embeddings go beyond mere statistical methods by capturing semantic relationships through training processes.\n",
      "\n",
      "In essence, embeddings transform complex data structures into manageable vector forms, enabling AI models to effectively process and analyze information across various domains.\n"
     ]
    }
   ],
   "source": [
    "# Call the Ollama model to generate a response  \n",
    "response = ollama.chat(\n",
    "    model=\"deepseek-r1:8b\",  \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain Embeddings and Vectors in AI.\"},  # User's input query\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(response[\"message\"][\"content\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d2b0f7-a3ec-418f-9807-20b6ddba252d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define the function that processes the PDF\n",
    "def process_pdf(pdf_bytes):\n",
    "    if pdf_bytes is None:\n",
    "        return None, None, None\n",
    "\n",
    "    loader = PyMuPDFLoader(pdf_bytes) \n",
    "    data = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "\n",
    "    vectorstore=Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=\"./chroma_db\")  # Example directory\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    \"\"\"\n",
    "    The function returns 3 objects\n",
    "        text_splitter → (Used to split new text in the same way as before)\n",
    "        vectorstore → (Holds the processed document chunks)\n",
    "        retriever → (Used to fetch relevant document chunks when answering questions)\n",
    "    \"\"\"\n",
    "    \n",
    "    return text_splitter, vectorstore, retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdce7c97-8c46-402f-a574-42139ad67509",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f5d31ca-42ba-4c0e-8f7b-5233e776dc98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ollama_llm(question, context):\n",
    "\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"deepseek-r1:8b\",  # Specifies the AI model to use\n",
    "        messages=[{'role': 'user', 'content': formatted_prompt}]  # Formats the user input\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "\n",
    "    final_answer = re.sub(r'<think>.*?</think>', # We're searching for think tags\n",
    "                          '', # We'll replace them with empty spaces\n",
    "                          response_content, # In response_content\n",
    "                          flags=re.DOTALL).strip() # (dot) should match newlines (\\n) as well.\n",
    "    # Return the final cleaned response\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf97fc9-c2a1-45c0-b716-56c5a9d30708",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define rag_chain function for Retrieval Augmented Generation\n",
    "def rag_chain(question, text_splitter, vectorstore, retriever):\n",
    "    \"\"\"\n",
    "    This function takes as input:\n",
    "        - The question we want to ask the model\n",
    "        - The text_splitter object to split the PDF and read into chunks\n",
    "        - The vectorstore for retrieving embeddings \n",
    "        - The retriever objects which retrieves data from the vectorstore\n",
    "    \"\"\"\n",
    "    retrieved_docs = retriever.invoke(question) \n",
    "    formatted_content = combine_docs(retrieved_docs) \n",
    "    return ollama_llm(question, formatted_content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e6a285-8920-4717-a33d-972c3e54e19d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ask_question(pdf_bytes, question): \n",
    "    text_splitter, vectorstore, retriever = process_pdf(pdf_bytes) # Process the PDF\n",
    "    if text_splitter is None:\n",
    "        return None  # No PDF uploaded    \n",
    "    result = rag_chain(question, text_splitter, vectorstore, retriever) # Return the results with RAG\n",
    "    #return {result}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbbdfcc2-466b-4190-8711-b9ebce57b1b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://67b58e32f2e6edc67a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://67b58e32f2e6edc67a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=ask_question,  \n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF (optional)\"),  \n",
    "        gr.Textbox(label=\"Ask a question\")  \n",
    "    ],\n",
    "    #outputs=\"text\",  \n",
    "    outputs=gr.Markdown(),\n",
    "    title=\"Ask questions about your PDF\",  \n",
    "    description=\"Use Ollama model to answer your questions about the uploaded PDF document.\",  \n",
    ")\n",
    "\n",
    "# Launch the Gradio interface to start the web-based app\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
